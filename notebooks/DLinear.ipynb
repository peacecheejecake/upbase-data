{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from transformers import AutoformerConfig, AutoformerModel\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from modeling_DLinear.models.DLinear import Model as DLinearModel\n",
    "from modeling_DLinear.utils.tools import EarlyStopping\n",
    "\n",
    "from upbit import UpbitCandles\n",
    "from utils.datetime import kst_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_COLUMN = 'candle_date_time_kst'\n",
    "TARGET_COLUMN = 'best_profit_rate'\n",
    "\n",
    "CHECKPOINT_PATH = '../model_checkpoints/simple_time_features'\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "  os.makedirs(CHECKPOINT_PATH)\n",
    "\n",
    "TRAIN_DATA_FILE_NAME = 'IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet'\n",
    "TEST_DATA_FILE_NAME = 'IOTA_1s_2000_2025-01-12T23:21:27+09:00.parquet_20250112232127.parquet'\n",
    "\n",
    "# Parameters\n",
    "# INPUT_LENGTH = 60  # Number of past time steps to use as input\n",
    "# OUTPUT_LENGTH = 12  # Number of future time steps to predict\n",
    "# BATCH_SIZE = 32\n",
    "# LEARNING_RATE = 5e-4\n",
    "# EPOCHS = 10\n",
    "\n",
    "SEQUENCE_LENGTH = 24 * 4 * 4\n",
    "PREDICTION_LENGTH = 24 * 4\n",
    "LABEL_LENGTH = 24 * 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES_PER_EPOCH = 100\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "SCALING = 'std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "  \n",
    "  default_path = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "  \n",
    "  @staticmethod\n",
    "  def load_parquet(file_name: str, file_dir: Optional[str] = None):\n",
    "    if not file_dir:\n",
    "        file_dir = DataUtils.default_path\n",
    "        \n",
    "    path = os.path.join(file_dir, file_name)\n",
    "\n",
    "    if not os.path.exists(path) or file_name.split('.')[-1] != 'parquet':\n",
    "        return\n",
    "\n",
    "    print(f'Loading parquet file from: {path}')\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Multivariate Time Series\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, features, time_features, sequence_length, label_length, prediction_length):\n",
    "        self.features = features\n",
    "        # self.target = target.flatten()\n",
    "        self.time_features = time_features\n",
    "        # self._make_time_features()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_length = label_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length - self.prediction_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_x = idx\n",
    "        end_x = idx + self.sequence_length\n",
    "        start_y = end_x - self.label_length\n",
    "        end_y = end_x + self.prediction_length\n",
    "        \n",
    "        x = self.features[start_x : end_x]\n",
    "        # mark_x = self.time_features[start_x : end_x].values\n",
    "        \n",
    "        y = self.features[start_y : end_y]\n",
    "        # mark_y = self.time_features[start_y : end_y].values\n",
    "        \n",
    "        return {\n",
    "            'x': torch.tensor(x, dtype=torch.float32),\n",
    "            'y': torch.tensor(y, dtype=torch.float32),\n",
    "            # 'mark_x': torch.tensor(mark_x, dtype=torch.float32),\n",
    "            # 'mark_y': torch.tensor(mark_y, dtype=torch.float32),\n",
    "        }\n",
    "        \n",
    "    def _make_time_features(self): \n",
    "        print(self.features, DATETIME_COLUMN)\n",
    "        df_stamp = pd.to_datetime(self.features[DATETIME_COLUMN])\n",
    "        df_stamp['month'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.month, 1)\n",
    "        df_stamp['day'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.day, 1)\n",
    "        df_stamp['weekday'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.weekday(), 1)\n",
    "        df_stamp['hour'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.hour, 1)\n",
    "        df_stamp['minute'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.minute, 1)\n",
    "        df_stamp['second'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.second, 1)\n",
    "        # self.time_features = df_stamp.drop([DATETIME_COLUMN], 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_features(data):\n",
    "    # Extract time-related features from the timestamp column\n",
    "    timestamps = pd.to_datetime(data[DATETIME_COLUMN])\n",
    "    time_features = pd.DataFrame({\n",
    "        # \"second_sin\": np.sin(2 * np.pi * timestamps.dt.second / 24),\n",
    "        # \"second_cos\": np.cos(2 * np.pi * timestamps.dt.second / 24),\n",
    "        # \"minute_sin\": np.sin(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        # \"minute_cos\": np.cos(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        # \"hour_sin\": np.sin(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        # \"hour_cos\": np.cos(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        # \"day_sin\": np.sin(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        # \"day_cos\": np.cos(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        # \"month_sin\": np.sin(2 * np.pi * timestamps.dt.month / 12),\n",
    "        # \"month_cos\": np.cos(2 * np.pi * timestamps.dt.month / 12),\n",
    "        'month': timestamps.dt.month,\n",
    "        'day': timestamps.dt.day,\n",
    "        'weekday': timestamps.dt.weekday,\n",
    "        'hour': timestamps.dt.hour,\n",
    "        'minute': timestamps.dt.minute,\n",
    "        'second': timestamps.dt.second,\n",
    "    })\n",
    "    return time_features\n",
    "\n",
    "def preprocess_data(data, feature_columns, sequence_length, label_length, prediction_length, test_size=0.2):\n",
    "    # Sort by timestamp if necessary\n",
    "    data = data.sort_values(DATETIME_COLUMN)\n",
    "\n",
    "    # Normalize the features and target\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "\n",
    "    df_features = data[feature_columns].dropna()\n",
    "    \n",
    "    features = df_features.drop(columns=[DATETIME_COLUMN, TARGET_COLUMN]).values\n",
    "    # features = data[feature_columns].values\n",
    "    target = df_features[TARGET_COLUMN].values.reshape(-1, 1)\n",
    "\n",
    "    features_normalized = scaler_features.fit_transform(features)\n",
    "    target_normalized = scaler_target.fit_transform(target)\n",
    "    data_normalized = np.concatenate([features_normalized, target_normalized], axis=1)\n",
    "    \n",
    "    time_features = generate_time_features(df_features)\n",
    "    NUM_TIME_FEATURES = time_features.shape[1]\n",
    "    \n",
    "    if test_size == 1:\n",
    "        test_dataset = TimeSeriesDataset(\n",
    "            data_normalized,\n",
    "            # target_normalized[~isnan.any(axis=1)],\n",
    "            time_features,\n",
    "            sequence_length=sequence_length,\n",
    "            prediction_length=prediction_length,\n",
    "            label_length=label_length,\n",
    "        )\n",
    "        return None, test_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES\n",
    "    \n",
    "    (\n",
    "        train_features, \n",
    "        val_features, \n",
    "        # train_target, \n",
    "        # val_target, \n",
    "        train_time_features, \n",
    "        val_time_features\n",
    "    ) = (\n",
    "        train_test_split(\n",
    "            data_normalized, \n",
    "            # target_normalized[~isnan.any(axis=1)], \n",
    "            time_features, \n",
    "            test_size=test_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_features,\n",
    "        # train_target,\n",
    "        train_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_features,\n",
    "        # val_target,\n",
    "        val_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES\n",
    "\n",
    "# def inverse_transform(data, predictions, feature_columns, scaler):\n",
    "#     features_dict = dict([key, data[key]] for key in feature_columns if key != TARGET_COLUMN)\n",
    "#     df = pd.DataFrame({\n",
    "#         **features_dict,\n",
    "#         TARGET_COLUMN: predictions,\n",
    "#     })\n",
    "#     return pd.DataFrame(scaler.inverse_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet file from: /Users/minjiwon/upbase-data-server/data/IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet\n"
     ]
    }
   ],
   "source": [
    "data = DataUtils.load_parquet(TRAIN_DATA_FILE_NAME)\n",
    "feature_columns = [\n",
    "    # 'best_profit_rate', \n",
    "    'variance', \n",
    "    'worst_profit_rate_before', \n",
    "    'opening_price', \n",
    "    'high_price', \n",
    "    'mid_price', \n",
    "    'low_price', \n",
    "    'candle_acc_trade_volume', \n",
    "    # 'diff_opening_price',\n",
    "    # 'diff_high_price',\n",
    "    # 'diff_mid_price',\n",
    "    # 'diff_low_price', \n",
    "    # 'diff_candle_acc_trade_volume',\n",
    "    'timedelta_after',\n",
    "    DATETIME_COLUMN,\n",
    "    # 'best_profit_rate',\n",
    "    TARGET_COLUMN,\n",
    "]\n",
    "# dataset = preprocess_data(data, feature_columns, 'best_profit_rate', 60, 10)\n",
    "  # data[[\n",
    "  #       'variance', \n",
    "  #       # 'best_profit_rate_before',\n",
    "  #       'worst_profit_rate_before', \n",
    "  #       'opening_price', \n",
    "  #       'high_price', \n",
    "  #       'mid_price', \n",
    "  #       'low_price', \n",
    "  #       'candle_acc_trade_volume', \n",
    "  #       # 'diff_opening_price',\n",
    "  #       # 'diff_high_price',\n",
    "  #       # 'diff_mid_price',\n",
    "  #       # 'diff_low_price', \n",
    "  #       # 'diff_candle_acc_trade_volume',\n",
    "  #       'timedelta_after',\n",
    "  #     ]],\n",
    "  #     data[['best_profit_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, _, _, NUM_TIME_FEATURES = preprocess_data(\n",
    "  data,\n",
    "  feature_columns,\n",
    "  sequence_length=SEQUENCE_LENGTH,\n",
    "  prediction_length=PREDICTION_LENGTH,\n",
    "  label_length=LABEL_LENGTH,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (decompsition): series_decomp(\n",
       "    (moving_avg): moving_avg(\n",
       "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (Linear_Seasonal): Linear(in_features=384, out_features=96, bias=True)\n",
       "  (Linear_Trend): Linear(in_features=384, out_features=96, bias=True)\n",
       "  (Linear_Decoder): Linear(in_features=384, out_features=96, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class DLinearConfig:\n",
    "  individual: bool = False\n",
    "  seq_len: int = SEQUENCE_LENGTH\n",
    "  pred_len: int = PREDICTION_LENGTH\n",
    "  label_len: int = LABEL_LENGTH\n",
    "  embed_type: int = 0\n",
    "  enc_in: int = 9\n",
    "  dec_in: int = 9\n",
    "  c_out: int = 9\n",
    "  d_model: int = 512\n",
    "  n_heads: int = 8\n",
    "  e_layers: int = 2\n",
    "  d_layers: int = 1\n",
    "  d_ff: int = 2048\n",
    "  moving_avg: int = 25\n",
    "  factor: int = 1\n",
    "  distill: bool = True\n",
    "  dropout: float = 0.1\n",
    "  activation: str = 'gelu'\n",
    "  output_attention: bool = False\n",
    "  embed: str = 'timeF'\n",
    "  do_predict: bool = False #whether to predict unseen future data\n",
    "  freq: str = 'ex' \n",
    "  \n",
    "\n",
    "configs = DLinearConfig()\n",
    "model = DLinearModel(configs)\n",
    "\n",
    "f_dim = -1\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "Validation loss decreased (inf --> 0.000036).  Saving model ...\n",
      "Learning rate: 0.0005 --> 0.001\n",
      "[EPOCH 2] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Learning rate: 0.001 --> 0.001\n",
      "[EPOCH 3] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Learning rate: 0.001 --> 0.0005\n",
      "[EPOCH 4] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Learning rate: 0.0005 --> 0.000125\n",
      "[EPOCH 5] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Learning rate: 0.000125 --> 1.5625e-05\n",
      "[EPOCH 6] 5898/5898 - Train Loss: 0.0001, Valid Loss: 0.0000\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "best_valid_loss = np.inf\n",
    "learning_rate = LEARNING_RATE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'\\r[EPOCH {epoch + 1}] {i + 1}/{len(train_loader)}', end=' ')\n",
    "        \n",
    "        def _get(key):\n",
    "            return batch[key].to(device)\n",
    "        \n",
    "        x = _get('x')\n",
    "        y = _get('y')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        \n",
    "        outputs = outputs[:, -PREDICTION_LENGTH:, f_dim:]\n",
    "        y = y[:, -PREDICTION_LENGTH:, f_dim:].to(device)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    # print()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            def _get(key):\n",
    "                return batch[key].to(device)\n",
    "            \n",
    "            x = _get('x')\n",
    "            y = _get('y')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            \n",
    "            outputs = outputs[:, -PREDICTION_LENGTH:, f_dim:]\n",
    "            y = y[:, -PREDICTION_LENGTH:, f_dim:].to(device)\n",
    "            \n",
    "            loss = criterion(outputs.detach().cpu(), y.detach().cpu())\n",
    "            valid_losses.append(loss)\n",
    "            \n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    \n",
    "    print(f'- Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     torch.save(model.state_dict(), '../model_checkpoints/DLinear.pth')\n",
    "    #     print('(Best model)')\n",
    "    # else:\n",
    "    #     print()\n",
    "       \n",
    "    early_stopping(valid_loss, model, CHECKPOINT_PATH)\n",
    "     \n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "        \n",
    "    prev_learning_rate = learning_rate\n",
    "    learning_rate *= (0.5 ** ((epoch - 1) // 1))\n",
    "    print(f'Learning rate: {prev_learning_rate} --> {learning_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "  \n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    # self.data = None\n",
    "    self.load_data()\n",
    "\n",
    "  # def _get_candles_from_upbit(self, count, to):\n",
    "  #   num_batches = count // 200 + math.ceil(count / 200)\n",
    "    \n",
    "  #   data = []\n",
    "    \n",
    "  #   for i in range(num_batches):\n",
    "  #     print(f'\\r{i + 1}/{num_batches}', end=\"\")\n",
    "\n",
    "  #     # url = f\"{base_url(market, candle_unit)}?market={market}&count={count}&to={str(to).split('.')[:-1][0]}\"\n",
    "  #     # response = requests.get(url, headers=headers)\n",
    "  #     # data += json.loads(response.text)\n",
    "  #     _count = min(count - len(data), 200)\n",
    "  #     data += UpbitCandles.get_candles('KRW-IOTA', 'second', to, _count)\n",
    "  #     print(data)\n",
    "  #     # to -= delta_to * count\n",
    "  #     last_datetime = datetime.strptime(data[-1]['candle_date_time_kst'], '%Y-%m-%dT%H:%M:%S')\n",
    "  #     to = kst_time(last_datetime - timedelta(seconds=1))\n",
    "\n",
    "  #     time.sleep(0.1)\n",
    "\n",
    "  #   print()\n",
    "\n",
    "  #   self.data = pd.DataFrame(data=data)\n",
    "\n",
    "  # def get_data(self):\n",
    "  #   # while True:\n",
    "  #   self._get_candles_from_upbit(count=2000, to='2024-10-15T18:19:37+09:00')\n",
    "  #   self.dataset, _, _ = preprocess_data(\n",
    "  #     data=data,\n",
    "  #     feature_columns=feature_columns,\n",
    "  #     target_column=TARGET_COLUMN,\n",
    "  #     sequence_length=SEQUENCE_LENGTH,\n",
    "  #     label_length=PREDICTION_LENGTH,\n",
    "  #     prediction_length=LABEL_LENGTH, \n",
    "  #     test_size=1\n",
    "  #   )\n",
    "    \n",
    "  def load_data(self):\n",
    "    self.data = DataUtils.load_parquet(TEST_DATA_FILE_NAME)\n",
    "    _, self.dataset, _, self.scaler_target, _ = preprocess_data(\n",
    "      data=self.data,\n",
    "      feature_columns=feature_columns,\n",
    "      # target_column=TARGET_COLUMN,\n",
    "      sequence_length=SEQUENCE_LENGTH,\n",
    "      label_length=PREDICTION_LENGTH,\n",
    "      prediction_length=LABEL_LENGTH, \n",
    "      test_size=1\n",
    "    )\n",
    "    \n",
    "  def data_loader(self):\n",
    "    return DataLoader(self.dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "  \n",
    "  def test(self):\n",
    "    self.model.eval()\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for i, batch in enumerate(self.data_loader()):\n",
    "        def _get(key):\n",
    "            return batch[key].to(device)\n",
    "          \n",
    "        x = _get('x')\n",
    "        y = _get('y')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        \n",
    "        outputs = outputs[:, -PREDICTION_LENGTH:, f_dim:]\n",
    "        y = y[:, -PREDICTION_LENGTH:, f_dim:].to(device)\n",
    "        \n",
    "        loss = criterion(outputs.detach().cpu(), y.detach().cpu())\n",
    "        test_losses.append(loss)\n",
    "      \n",
    "    print(f'TEST LOSS: {np.average(test_losses)}')\n",
    "    return np.average(test_losses)\n",
    "\n",
    "  def test_sample(self):\n",
    "    self.model.eval()\n",
    "    \n",
    "    output_list = []\n",
    "    truth_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for i, batch in enumerate(self.data_loader()):\n",
    "        def _get(key):\n",
    "            return batch[key].to(device)\n",
    "            \n",
    "        x = _get('x')\n",
    "        y = _get('y')\n",
    "        \n",
    "        outputs = model(x)[:, -PREDICTION_LENGTH:, f_dim:]\n",
    "        y = y[:, -PREDICTION_LENGTH:, f_dim:].to(device)\n",
    "        \n",
    "        output_list += list(outputs.detach().cpu().flatten().numpy())\n",
    "        truth_list += list(y.detach().cpu().flatten().numpy())\n",
    "      \n",
    "    out = self.scaler_target.inverse_transform(np.array(output_list).reshape(-1, 1))\n",
    "    truth = self.scaler_target.inverse_transform(np.array(truth_list).reshape(-1, 1))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "      'output': out.flatten(),\n",
    "      'truth': truth.flatten(),\n",
    "      'diff': out.flatten() - truth.flatten(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet file from: /Users/minjiwon/upbase-data-server/data/IOTA_1s_2000_2025-01-12T23:21:27+09:00.parquet_20250112232127.parquet\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{CHECKPOINT_PATH}/checkpoint.pth'))\n",
    "tester = Tester(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS: 0.04067673906683922\n"
     ]
    }
   ],
   "source": [
    "tester.test()\n",
    "df = tester.test_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>truth</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000092</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134491</th>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134492</th>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>-0.002338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134493</th>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>-0.001805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134494</th>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>-0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134495</th>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>-0.002981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134496 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          output     truth      diff\n",
       "0       0.000125  0.000199 -0.000074\n",
       "1      -0.000264 -0.000099 -0.000165\n",
       "2      -0.000032  0.000298 -0.000330\n",
       "3      -0.000194  0.000298 -0.000492\n",
       "4      -0.000092  0.000298 -0.000390\n",
       "...          ...       ...       ...\n",
       "134491  0.000915  0.002168 -0.001253\n",
       "134492  0.001017  0.003354 -0.002338\n",
       "134493  0.000956  0.002761 -0.001805\n",
       "134494  0.001043  0.003750 -0.002708\n",
       "134495  0.000967  0.003949 -0.002981\n",
       "\n",
       "[134496 rows x 3 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002: 0.07012847965738758\n",
      "0.0001: 0.03499732334047109\n",
      "5e-05: 0.017814656197953842\n",
      "1e-05: 0.003643231025458006\n"
     ]
    }
   ],
   "source": [
    "thresholds = [2e-4, 1e-4, 5e-5, 1e-5]\n",
    "abs_diff = df['diff'].abs()\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f'{threshold}: {len(abs_diff[abs_diff <= threshold]) / len(abs_diff)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
