{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from transformers import AutoformerConfig, AutoformerModel\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from utils.time_features import time_features\n",
    "from modeling_DLinear.models.Autoformer import Model as AutoFormerModel\n",
    "from modeling_DLinear.utils.tools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_COLUMN = 'candle_date_time_kst'\n",
    "\n",
    "TARGET_COLUMN = 'best_profit_rate'\n",
    "\n",
    "# Parameters\n",
    "# INPUT_LENGTH = 60  # Number of past time steps to use as input\n",
    "# OUTPUT_LENGTH = 12  # Number of future time steps to predict\n",
    "# BATCH_SIZE = 32\n",
    "# LEARNING_RATE = 5e-4\n",
    "# EPOCHS = 10\n",
    "\n",
    "SEQUENCE_LENGTH = 24 * 4 * 4\n",
    "PREDICTION_LENGTH = 24 * 4\n",
    "LABEL_LENGTH = 24 * 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES_PER_EPOCH = 100\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "SCALING = 'std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "  \n",
    "  default_path = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "  \n",
    "  @staticmethod\n",
    "  def load_parquet(file_name: str, file_dir: Optional[str] = None):\n",
    "    if not file_dir:\n",
    "        file_dir = DataUtils.default_path\n",
    "        \n",
    "    path = os.path.join(file_dir, file_name)\n",
    "\n",
    "    if not os.path.exists(path) or file_name.split('.')[-1] != 'parquet':\n",
    "        return\n",
    "\n",
    "    print(f'Loading parquet file from: {path}')\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Multivariate Time Series\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, features, target, time_features, sequence_length, label_length, prediction_length):\n",
    "        self.features = features\n",
    "        self.target = target.flatten()\n",
    "        self.time_features = time_features\n",
    "        # self._make_time_features()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_length = label_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length - self.prediction_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_x = idx\n",
    "        end_x = idx + self.sequence_length\n",
    "        start_y = end_x - self.label_length\n",
    "        end_y = end_x + self.prediction_length\n",
    "        \n",
    "        x = self.features[start_x : end_x]\n",
    "        mark_x = self.time_features[start_x : end_x].values\n",
    "        \n",
    "        # future_values = self.features[start_y : end_y]\n",
    "        # y = self.target[start_y : end_y].reshape(1, -1)\n",
    "        # y = np.array([self.target[i : i + self.label_length] for i in range(start_y, end_y)])\n",
    "        y = self.features[start_y : end_y]\n",
    "        mark_y = self.time_features[start_y : end_y].values\n",
    "        # print('&&&& ', self.target.shape, y.shape)\n",
    "        # y = self.target[idx : idx + self.sequence_length]\n",
    "        \n",
    "        # past_observed_mask = np.ones((self.sequence_length, past_values.shape[1]))\n",
    "        # future_observed_mask = np.ones((self.sequence_length, future_values.shape[1]))\n",
    "        \n",
    "        # print(past_values.shape, past_time_features.shape, future_time_features.shape, past_observed_mask.shape)\n",
    "        \n",
    "        return {\n",
    "            # 'past_values': torch.tensor(past_values, dtype=torch.float32), \n",
    "            # 'past_time_features': torch.tensor(past_time_features, dtype=torch.float32),\n",
    "            # 'past_observed_mask': torch.tensor(past_observed_mask, dtype=torch.bool),\n",
    "            # 'future_values': torch.tensor(future_values, dtype=torch.float32), \n",
    "            # 'future_time_features': torch.tensor(future_time_features, dtype=torch.float32),\n",
    "            # 'future_observed_mask': torch.tensor(future_observed_mask, dtype=torch.bool),\n",
    "            'x': torch.tensor(x, dtype=torch.float32),\n",
    "            'y': torch.tensor(y, dtype=torch.float32),\n",
    "            'mark_x': torch.tensor(mark_x, dtype=torch.float32),\n",
    "            'mark_y': torch.tensor(mark_y, dtype=torch.float32),\n",
    "        }\n",
    "        \n",
    "    def _make_time_features(self): \n",
    "        print(self.features, DATETIME_COLUMN)\n",
    "        df_stamp = pd.to_datetime(self.features[DATETIME_COLUMN])\n",
    "        df_stamp['month'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.month, 1)\n",
    "        df_stamp['day'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.day, 1)\n",
    "        df_stamp['weekday'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.weekday(), 1)\n",
    "        df_stamp['hour'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.hour, 1)\n",
    "        df_stamp['minute'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.minute, 1)\n",
    "        df_stamp['second'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.second, 1)\n",
    "        # self.time_features = df_stamp.drop([DATETIME_COLUMN], 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_features(data):\n",
    "    # Extract time-related features from the timestamp column\n",
    "    timestamps = pd.to_datetime(data[DATETIME_COLUMN])\n",
    "    time_features = pd.DataFrame({\n",
    "        \"second_sin\": np.sin(2 * np.pi * timestamps.dt.second / 24),\n",
    "        \"second_cos\": np.cos(2 * np.pi * timestamps.dt.second / 24),\n",
    "        \"minute_sin\": np.sin(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        \"minute_cos\": np.cos(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        \"hour_sin\": np.sin(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        \"hour_cos\": np.cos(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        \"day_sin\": np.sin(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        \"day_cos\": np.cos(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        \"month_sin\": np.sin(2 * np.pi * timestamps.dt.month / 12),\n",
    "        \"month_cos\": np.cos(2 * np.pi * timestamps.dt.month / 12),\n",
    "    })\n",
    "    return time_features\n",
    "\n",
    "def preprocess_data(data, feature_columns, target_column, sequence_length, label_length, prediction_length):\n",
    "    # Sort by timestamp if necessary\n",
    "    data = data.sort_values(DATETIME_COLUMN)\n",
    "\n",
    "    # Normalize the features and target\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "\n",
    "    # features = data.drop(columns=[target_column]).values\n",
    "    features = data[feature_columns].values\n",
    "    target = data[target_column].values.reshape(-1, 1)\n",
    "\n",
    "    features_normalized = scaler_features.fit_transform(features)\n",
    "    target_normalized = scaler_target.fit_transform(target)\n",
    "    isnan = np.isnan(features_normalized)\n",
    "    # print(features_normalized.shape, isnan.shape, features_normalized[~isnan.any(axis=1)].shape, target_normalized[~isnan.any(axis=1)].shape)\n",
    "    \n",
    "    # Generate time-based features\n",
    "    time_features = generate_time_features(data=data)\n",
    "    NUM_TIME_FEATURES = time_features.shape[1]\n",
    "    # print(features.shape, target.shape, time_features.shape)\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    (\n",
    "        train_features, \n",
    "        val_features, \n",
    "        train_target, \n",
    "        val_target, \n",
    "        train_time_features, \n",
    "        val_time_features\n",
    "    ) = (\n",
    "        train_test_split(\n",
    "            features_normalized[~isnan.any(axis=1)], \n",
    "            target_normalized[~isnan.any(axis=1)], \n",
    "            time_features[~isnan.any(axis=1)], \n",
    "            test_size=0.2, \n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "    # train_features, val_features, train_target, val_target = train_test_split(\n",
    "    #     features_normalized, target_normalized, test_size=0.2, shuffle=False\n",
    "    # )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_features,\n",
    "        train_target,\n",
    "        train_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_features,\n",
    "        val_target,\n",
    "        val_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet file from: /Users/minjiwon/upbase-data-server/data/IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet\n"
     ]
    }
   ],
   "source": [
    "data = DataUtils.load_parquet('IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet')\n",
    "feature_columns = [\n",
    "    # 'best_profit_rate', \n",
    "    'variance', \n",
    "    'worst_profit_rate_before', \n",
    "    'opening_price', \n",
    "    'high_price', \n",
    "    'mid_price', \n",
    "    'low_price', \n",
    "    'candle_acc_trade_volume', \n",
    "    # 'diff_opening_price',\n",
    "    # 'diff_high_price',\n",
    "    # 'diff_mid_price',\n",
    "    # 'diff_low_price', \n",
    "    # 'diff_candle_acc_trade_volume',\n",
    "    'timedelta_after',\n",
    "    'best_profit_rate',\n",
    "]\n",
    "# dataset = preprocess_data(data, feature_columns, 'best_profit_rate', 60, 10)\n",
    "  # data[[\n",
    "  #       'variance', \n",
    "  #       # 'best_profit_rate_before',\n",
    "  #       'worst_profit_rate_before', \n",
    "  #       'opening_price', \n",
    "  #       'high_price', \n",
    "  #       'mid_price', \n",
    "  #       'low_price', \n",
    "  #       'candle_acc_trade_volume', \n",
    "  #       # 'diff_opening_price',\n",
    "  #       # 'diff_high_price',\n",
    "  #       # 'diff_mid_price',\n",
    "  #       # 'diff_low_price', \n",
    "  #       # 'diff_candle_acc_trade_volume',\n",
    "  #       'timedelta_after',\n",
    "  #     ]],\n",
    "  #     data[['best_profit_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES = preprocess_data(\n",
    "  data,\n",
    "  feature_columns,\n",
    "  TARGET_COLUMN,\n",
    "  sequence_length=SEQUENCE_LENGTH,\n",
    "  prediction_length=PREDICTION_LENGTH,\n",
    "  label_length=LABEL_LENGTH,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AutoFormerConfig:\n",
    "  seq_len: int = SEQUENCE_LENGTH\n",
    "  pred_len: int = PREDICTION_LENGTH\n",
    "  label_len: int = LABEL_LENGTH\n",
    "  embed_type: int = 0\n",
    "  enc_in: int = 9\n",
    "  dec_in: int = 9\n",
    "  c_out: int = 9\n",
    "  d_model: int = 512\n",
    "  n_heads: int = 8\n",
    "  e_layers: int = 2\n",
    "  d_layers: int = 1\n",
    "  d_ff: int = 2048\n",
    "  moving_avg: int = 25\n",
    "  factor: int = 1\n",
    "  distill: bool = True\n",
    "  dropout: float = 0.1\n",
    "  activation: str = 'gelu'\n",
    "  output_attention: bool = False\n",
    "  embed: str = 'timeF'\n",
    "  do_predict: bool = False #whether to predict unseen future data\n",
    "  freq: str = 'ex' \n",
    "  \n",
    "\n",
    "configs = AutoFormerConfig()\n",
    "model = AutoFormerModel(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Batch 176/5898"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(x.shape, y.shape, mark_x.shape, mark_y.shape, x_dec.shape)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print('!1', x_dec.shape, y.shape)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmark_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmark_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(outputs.shape, y.shape, y)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m f_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/models/Autoformer.py:107\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# enc\u001b[39;00m\n\u001b[1;32m    106\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_embedding(x_enc, x_mark_enc)\n\u001b[0;32m--> 107\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_self_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# dec\u001b[39;00m\n\u001b[1;32m    109\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(seasonal_init, x_mark_dec)\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/layers/Autoformer_EncDec.py:103\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_layers:\n\u001b[0;32m--> 103\u001b[0m         x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m         attns\u001b[38;5;241m.\u001b[39mappend(attn)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/layers/Autoformer_EncDec.py:69\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 69\u001b[0m     new_x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(new_x)\n\u001b[1;32m     74\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomp1(x)\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/layers/AutoCorrelation.py:151\u001b[0m, in \u001b[0;36mAutoCorrelationLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    148\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(keys)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    149\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_correlation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_projection(out), attn\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/layers/AutoCorrelation.py:117\u001b[0m, in \u001b[0;36mAutoCorrelation.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# time delay agg\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 117\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_delay_agg_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_delay_agg_inference(values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous(), corr)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/upbase-data-server/notebooks/modeling_DLinear/layers/AutoCorrelation.py:33\u001b[0m, in \u001b[0;36mAutoCorrelation.time_delay_agg_training\u001b[0;34m(self, values, corr)\u001b[0m\n\u001b[1;32m     31\u001b[0m mean_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mmean(corr, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(torch\u001b[38;5;241m.\u001b[39mmean(mean_value, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), top_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mean_value[:, index[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(top_k)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# update corr\u001b[39;00m\n\u001b[1;32m     35\u001b[0m tmp_corr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(f'EPOCH {epoch + 1}')\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'\\rBatch {i + 1}/{len(train_loader)}', end='')\n",
    "        \n",
    "        def _get(key):\n",
    "            return batch[key].to(device)\n",
    "        \n",
    "        x = _get('x')\n",
    "        y = _get('y')\n",
    "        mark_x = _get('mark_x')\n",
    "        mark_y = _get('mark_y')\n",
    "        \n",
    "        # decoder input\n",
    "        x_dec = torch.zeros_like(y[:, -PREDICTION_LENGTH:, :]).float()\n",
    "        x_dec = torch.cat([y[:, :LABEL_LENGTH, :], x_dec], dim=1).float().to(device)\n",
    "        # print(x.shape, y.shape, mark_x.shape, mark_y.shape, x_dec.shape)\n",
    "        # print('!1', x_dec.shape, y.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, mark_x, x_dec, mark_y)\n",
    "        # print(outputs.shape, y.shape, y)\n",
    "        \n",
    "        f_dim = -1\n",
    "        outputs = outputs[:, -PREDICTION_LENGTH:, f_dim:]\n",
    "        y = y[:, -PREDICTION_LENGTH:, f_dim:].to(device)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    model.eval()\n",
    "    # val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            def _get(key):\n",
    "                return batch[key].to(device)\n",
    "            \n",
    "            x = _get('x')\n",
    "            y = _get('y')\n",
    "            mark_x = _get('mark_x')\n",
    "            mark_y = _get('mark_y')\n",
    "            \n",
    "            # decoder input\n",
    "            x_dec = torch.zeros_like(y[:, -PREDICTION_LENGTH:, :]).float()\n",
    "            x_dec = torch.cat([y[:, :LABEL_LENGTH, :], x_dec], dim=1).float().to(device)\n",
    "            # print('!1', dec_inp.shape, y.shape)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, mark_x, x_dec, mark_y)\n",
    "            loss = criterion(outputs.detach().cpu(), y.detach().cpu())\n",
    "            valid_losses.append(loss)\n",
    "            \n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./autoformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
