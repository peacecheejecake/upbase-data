{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoformerConfig, AutoformerModel\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from utils.time_features import time_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_COLUMN = 'candle_date_time_kst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "  \n",
    "  default_path = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "  \n",
    "  @staticmethod\n",
    "  def load_parquet(file_name: str, file_dir: Optional[str] = None):\n",
    "    if not file_dir:\n",
    "        file_dir = DataUtils.default_path\n",
    "        \n",
    "    path = os.path.join(file_dir, file_name)\n",
    "\n",
    "    if not os.path.exists(path) or file_name.split('.')[-1] != 'parquet':\n",
    "        return\n",
    "\n",
    "    print(f'Loading parquet file from: {path}')\n",
    "\n",
    "    return pd.read_parquet(path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Multivariate Time Series\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, features, target, time_features, sequence_length, label_length, prediction_length):\n",
    "        self.features = features\n",
    "        self.target = target.flatten()\n",
    "        # self.time_features = time_features(pd.to_datetime(features[DATETIME_COLUMN].values), freq='H')\n",
    "        # self.time_features = self._make_time_features(features)\n",
    "        self.time_features = time_features\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_length = label_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length - self.prediction_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_past = idx\n",
    "        end_past = idx + self.sequence_length\n",
    "        start_future = end_past - self.label_length\n",
    "        end_future = end_past + self.prediction_length\n",
    "        \n",
    "        past_values = self.features[start_past : end_past]\n",
    "        past_time_features = self.time_features[start_past : end_past]\n",
    "        \n",
    "        future_values = self.features[start_future : end_future]\n",
    "        y = self.target[start_future : end_future]\n",
    "        future_time_features = self.time_features[start_future : end_future]\n",
    "        # print('&&&& ', self.target.shape, y.shape)\n",
    "        # y = self.target[idx : idx + self.sequence_length]\n",
    "        \n",
    "        past_observed_mask = np.ones((self.sequence_length, past_values.shape[1]))\n",
    "        future_observed_mask = np.ones((self.sequence_length, future_values.shape[1]))\n",
    "        \n",
    "        # print(past_values.shape, past_time_features.shape, future_time_features.shape, past_observed_mask.shape)\n",
    "        \n",
    "        return {\n",
    "            'past_values': torch.tensor(past_values, dtype=torch.float32), \n",
    "            'past_time_features': torch.tensor(past_time_features, dtype=torch.float32),\n",
    "            'past_observed_mask': torch.tensor(past_observed_mask, dtype=torch.bool),\n",
    "            'future_values': torch.tensor(future_values, dtype=torch.float32), \n",
    "            'future_time_features': torch.tensor(future_time_features, dtype=torch.float32),\n",
    "            'future_observed_mask': torch.tensor(future_observed_mask, dtype=torch.bool),\n",
    "            'y': torch.tensor(y, dtype=torch.float32),\n",
    "        }\n",
    "        \n",
    "    # def _make_time_features(self, features: pd.DataFrame): \n",
    "    #     df_stamp = features[[DATETIME_COLUMN]]\n",
    "    #     df_stamp['month'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.month, 1)\n",
    "    #     df_stamp['day'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.day, 1)\n",
    "    #     df_stamp['weekday'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.weekday(), 1)\n",
    "    #     df_stamp['hour'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.hour, 1)\n",
    "    #     df_stamp['minute'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.minute, 1)\n",
    "    #     df_stamp['second'] = df_stamp[DATETIME_COLUMN].apply(lambda row: row.second, 1)\n",
    "    #     data_stamp = df_stamp.drop([DATETIME_COLUMN], 1).values\n",
    "    #     return data_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_features(data):\n",
    "    # Extract time-related features from the timestamp column\n",
    "    timestamps = pd.to_datetime(data[DATETIME_COLUMN])\n",
    "    time_features = pd.DataFrame({\n",
    "        \"second_sin\": np.sin(2 * np.pi * timestamps.dt.second / 24),\n",
    "        \"second_cos\": np.cos(2 * np.pi * timestamps.dt.second / 24),\n",
    "        \"minute_sin\": np.sin(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        \"minute_cos\": np.cos(2 * np.pi * timestamps.dt.minute / 24),\n",
    "        \"hour_sin\": np.sin(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        \"hour_cos\": np.cos(2 * np.pi * timestamps.dt.hour / 24),\n",
    "        \"day_sin\": np.sin(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        \"day_cos\": np.cos(2 * np.pi * timestamps.dt.day / timestamps.dt.days_in_month),\n",
    "        \"month_sin\": np.sin(2 * np.pi * timestamps.dt.month / 12),\n",
    "        \"month_cos\": np.cos(2 * np.pi * timestamps.dt.month / 12),\n",
    "    })\n",
    "    return time_features\n",
    "\n",
    "def preprocess_data(data, feature_columns, target_column, sequence_length, label_length, prediction_length):\n",
    "    # Sort by timestamp if necessary\n",
    "    data = data.sort_values(DATETIME_COLUMN)\n",
    "\n",
    "    # Normalize the features and target\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "\n",
    "    # features = data.drop(columns=[target_column]).values\n",
    "    features = data[feature_columns].values\n",
    "    target = data[target_column].values.reshape(-1, 1)\n",
    "\n",
    "    features_normalized = scaler_features.fit_transform(features)\n",
    "    target_normalized = scaler_target.fit_transform(target)\n",
    "    isnan = np.isnan(features_normalized)\n",
    "    # print(features_normalized.shape, isnan.shape, features_normalized[~isnan.any(axis=1)].shape, target_normalized[~isnan.any(axis=1)].shape)\n",
    "    \n",
    "    # Generate time-based features\n",
    "    time_features = generate_time_features(data=data).values\n",
    "    NUM_TIME_FEATURES = time_features.shape[1]\n",
    "    # print(features.shape, target.shape, time_features.shape)\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    (\n",
    "        train_features, \n",
    "        val_features, \n",
    "        train_target, \n",
    "        val_target, \n",
    "        train_time_features, \n",
    "        val_time_features\n",
    "    ) = (\n",
    "        train_test_split(\n",
    "            features_normalized[~isnan.any(axis=1)], \n",
    "            target_normalized[~isnan.any(axis=1)], \n",
    "            time_features[~isnan.any(axis=1)], \n",
    "            test_size=0.2, \n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "    # train_features, val_features, train_target, val_target = train_test_split(\n",
    "    #     features_normalized, target_normalized, test_size=0.2, shuffle=False\n",
    "    # )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_features,\n",
    "        train_target,\n",
    "        train_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_features,\n",
    "        val_target,\n",
    "        val_time_features,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_length=prediction_length,\n",
    "        label_length=label_length,\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet file from: /Users/minjiwon/upbase-data-server/data/IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet\n"
     ]
    }
   ],
   "source": [
    "data = DataUtils.load_parquet('IOTA_1s_2000000_2025-01-05T18:19:37+09:00.parquet_20250105181937.parquet')\n",
    "feature_columns = [\n",
    "    # 'best_profit_rate', \n",
    "    'variance', \n",
    "    'worst_profit_rate_before', \n",
    "    'opening_price', \n",
    "    'high_price', \n",
    "    'mid_price', \n",
    "    'low_price', \n",
    "    'candle_acc_trade_volume', \n",
    "    # 'diff_opening_price',\n",
    "    # 'diff_high_price',\n",
    "    # 'diff_mid_price',\n",
    "    # 'diff_low_price', \n",
    "    # 'diff_candle_acc_trade_volume',\n",
    "    'timedelta_after',\n",
    "]\n",
    "# dataset = preprocess_data(data, feature_columns, 'best_profit_rate', 60, 10)\n",
    "  # data[[\n",
    "  #       'variance', \n",
    "  #       # 'best_profit_rate_before',\n",
    "  #       'worst_profit_rate_before', \n",
    "  #       'opening_price', \n",
    "  #       'high_price', \n",
    "  #       'mid_price', \n",
    "  #       'low_price', \n",
    "  #       'candle_acc_trade_volume', \n",
    "  #       # 'diff_opening_price',\n",
    "  #       # 'diff_high_price',\n",
    "  #       # 'diff_mid_price',\n",
    "  #       # 'diff_low_price', \n",
    "  #       # 'diff_candle_acc_trade_volume',\n",
    "  #       'timedelta_after',\n",
    "  #     ]],\n",
    "  #     data[['best_profit_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'best_profit_rate'\n",
    "\n",
    "# Parameters\n",
    "# INPUT_LENGTH = 60  # Number of past time steps to use as input\n",
    "# OUTPUT_LENGTH = 12  # Number of future time steps to predict\n",
    "# BATCH_SIZE = 32\n",
    "# LEARNING_RATE = 5e-4\n",
    "# EPOCHS = 10\n",
    "\n",
    "SEQUENCE_LENGTH = 24 * 4 * 4\n",
    "PREDICTION_LENGTH = 24 * 4\n",
    "LABEL_LENGTH = 24 * 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES_PER_EPOCH = 100\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "SCALING = 'std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "train_dataset, val_dataset, scaler_features, scaler_target, NUM_TIME_FEATURES = preprocess_data(\n",
    "  data,\n",
    "  feature_columns,\n",
    "  TARGET_COLUMN,\n",
    "  sequence_length=SEQUENCE_LENGTH,\n",
    "  prediction_length=PREDICTION_LENGTH,\n",
    "  label_length=LABEL_LENGTH,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Autoformer Model\n",
    "configuration = AutoformerConfig(\n",
    "    input_size=len(feature_columns),  # Number of input features\n",
    "    prediction_length=PREDICTION_LENGTH,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    label_length=LABEL_LENGTH,\n",
    "    # encoder_layers=3,\n",
    "    # decoder_layers=3,\n",
    "    # d_model=32,\n",
    "    lags_sequence=[0],\n",
    "    num_time_features=NUM_TIME_FEATURES,\n",
    "    # lags_sequence=list(range(1, 8)),  # Ensure lags are within input_length\n",
    ")\n",
    "model = AutoformerModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "torch.Size([128, 384, 8]) torch.Size([128, 384, 10]) torch.Size([128, 384, 8])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input length 480 and time feature lengths 576 does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(_get(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_values\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshape, _get(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_time_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshape, _get(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_observed_mask\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# x.unsqueeze(-1),\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpast_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpast_time_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Placeholder for time-based features\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpast_observed_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Mask indicating observed data\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfuture_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfuture_time_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# future_observed_mask=_get('future_observed_mask'),\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mkeys(), y\u001b[38;5;241m.\u001b[39mshape, y)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_last_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_features\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# for key in ['encoder_last_hidden_state', 'loc', 'scale', 'static_features']:\u001b[39;00m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/transformers/models/autoformer/modeling_autoformer.py:1660\u001b[0m, in \u001b[0;36mAutoformerModel.forward\u001b[0;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[1;32m   1657\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   1658\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1660\u001b[0m transformer_inputs, temporal_features, loc, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1671\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m   1672\u001b[0m         (\n\u001b[1;32m   1673\u001b[0m             transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1677\u001b[0m     )\n",
      "File \u001b[0;32m~/upbase-data-server/.venv/lib/python3.12/site-packages/transformers/models/autoformer/modeling_autoformer.py:1590\u001b[0m, in \u001b[0;36mAutoformerModel.create_network_inputs\u001b[0;34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[0m\n\u001b[1;32m   1587\u001b[0m reshaped_lagged_sequence \u001b[38;5;241m=\u001b[39m lagged_sequence\u001b[38;5;241m.\u001b[39mreshape(lags_shape[\u001b[38;5;241m0\u001b[39m], lags_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reshaped_lagged_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m time_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 1590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1591\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreshaped_lagged_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and time feature lengths \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reshaped_lagged_sequence, features, loc, scale, static_feat\n",
      "\u001b[0;31mValueError\u001b[0m: input length 480 and time feature lengths 576 does not match"
     ]
    }
   ],
   "source": [
    "# Optimizer and Loss Function\n",
    "device = torch.device(DEVICE)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    print(f'EPOCH {epoch + 1}')\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # past_values, future_values, y, time_features, mask = (_d.to(device) for _d in _data)\n",
    "        def _get(key):\n",
    "            return batch[key].to(device)\n",
    "        \n",
    "        # decoder input\n",
    "        y = _get('y')\n",
    "        # dec_inp = torch.zeros_like(y[:, -PREDICTION_LENGTH:, :]).float()\n",
    "        # dec_inp = torch.cat([y[:, :LABEL_LENGTH, :], dec_inp], dim=1).float().to(device)\n",
    "        # print('!1', dec_inp.shape, y.shape)\n",
    "        \n",
    "        # print(batch['past_values'].shape, batch['future_values'].shape, batch['y'].shape, batch['past_time_features'].shape, batch['future_time_features'].shape, batch['past_observed_mask'].shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(_get('past_values').shape, _get('past_time_features').shape, _get('past_observed_mask').shape)\n",
    "        outputs = model(\n",
    "            # x.unsqueeze(-1),\n",
    "            past_values=_get('past_values'),\n",
    "            past_time_features=_get('past_time_features'),  # Placeholder for time-based features\n",
    "            past_observed_mask=_get('past_observed_mask'),  # Mask indicating observed data\n",
    "            future_values=_get('future_values'),\n",
    "            future_time_features=_get('future_time_features'), \n",
    "            # future_observed_mask=_get('future_observed_mask'),\n",
    "            return_dict=True,\n",
    "        )\n",
    "        print(outputs.keys(), y.shape, y)\n",
    "        for key in ['last_hidden_state', 'trend', 'encoder_last_hidden_state', 'loc', 'scale', 'static_features']:\n",
    "        # for key in ['encoder_last_hidden_state', 'loc', 'scale', 'static_features']:\n",
    "            print(key, outputs[key].shape if isinstance(outputs[key], torch.Tensor)  else outputs[key])\n",
    "            \n",
    "        print(outputs.trend)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            def _get(key):\n",
    "                return batch[key].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                # x.unsqueeze(-1),\n",
    "                past_values=_get('past_values'),\n",
    "                past_time_features=_get('past_time_features'),  # Placeholder for time-based features\n",
    "                future_values=_get('future_values'),\n",
    "                future_time_features=_get('future_time_features'), \n",
    "                past_observed_mask=_get('past_observed_mask'),  # Mask indicating observed data\n",
    "            )\n",
    "            loss = criterion(outputs, _get('y'))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./autoformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
